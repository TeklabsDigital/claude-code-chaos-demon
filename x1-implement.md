# X1 Implement

**Report:** "Using X1 Implementation methodology with completeness tracking."

## Instructions

1. Identify the approved plan (ask if unclear)
2. Extract the Implementation Checklist (all IMP-NNN items)
3. Display the checklist with count: "Found N items to implement"
4. Implement systematically â€” update TodoWrite with each item
5. After implementing, self-verify EVERY item against the codebase
6. Report completion status â€” cannot declare done with unresolved items

**Goal:** 100% implementation of the approved plan. No gaps. No silent skips.

**Speed:** Work in parallel wherever possible â€” read multiple files simultaneously, implement independent items concurrently.

---

## Phase 1: Checklist Extraction

Read the plan and extract ALL IMP-NNN items into a tracking table:

| ID | Category | Description | Status |
|----|----------|-------------|--------|
| IMP-001 | File | Create `path/to/file` | â¬œ Pending |
| IMP-002 | Feature | Implement X | â¬œ Pending |
| ... | ... | ... | ... |

**Count:** "Extracted N items. Beginning implementation."

If no Implementation Checklist exists in the plan, STOP and tell the user: "This plan has no Implementation Checklist. Run `/x1-plan` to create a plan with numbered IMP items first."

---

## Phase 2: Systematic Implementation

For each IMP item:
1. Mark as ğŸ”„ In Progress (update TodoWrite)
2. Implement the item
3. Verify it works (build, basic test)
4. Mark as âœ… Done

**Rules:**
- Implement in dependency order (infrastructure â†’ business logic â†’ tests)
- Do NOT skip items silently. If an item cannot be implemented, mark it â­ï¸ Skipped with a justification and **inform the user immediately**
- Do NOT batch â€” complete each item before starting the next
- Update progress after EVERY item: "Completed IMP-003 (5/47)"

---

## Phase 3: Self-Verification (MANDATORY â€” DO NOT SKIP)

After implementing all items, verify EVERY item against the codebase:

For each IMP item:
1. Does the file/function/test actually exist?
2. Does it match the plan's specification?
3. Is it complete (not a stub, not partial)?

Update status:
- âœ… Verified â€” exists and matches plan
- âš ï¸ Partial â€” exists but incomplete
- âŒ Missing â€” not found in codebase
- â­ï¸ Skipped â€” intentionally deferred (justification required)
- â– N/A â€” requirement changed during implementation (justification required)

---

## Phase 4: Completion Gate (BLOCKING)

**You CANNOT declare implementation complete until this gate passes.**

### Gate Criteria
1. Every IMP item has a status (no â¬œ Pending items remain)
2. Zero âŒ Missing items
3. Zero âš ï¸ Partial items
4. All â­ï¸ Skipped items have documented justification
5. All â– N/A items have documented justification
6. Tests pass (if tests were in the checklist)

### Completion Report (Gate PASSED)

```
## Implementation Complete

**Plan:** [plan name/path]
**Total Items:** N
**Status:**
- âœ… Implemented: X
- â­ï¸ Skipped (justified): Y
- â– N/A: Z

### Skipped Items (if any)
- IMP-XXX: [reason for skipping]

### Verification: All implemented items verified against codebase.
**Gate Status: PASSED**
```

### If Gate FAILS

```
## Implementation Incomplete â€” Gate FAILED

**Remaining:**
- âŒ IMP-XXX: [description] â€” Not implemented
- âš ï¸ IMP-YYY: [description] â€” Partial: [what's missing]

**Action Required:** Implement remaining items before declaring done.
```

Then continue implementing the remaining items. Re-run the gate after each pass.

---

## Red Flags During Implementation

- ğŸš© Skipping an item "because it's simple" â€” implement it anyway
- ğŸš© "Will do this after" â€” do it now, it's in the checklist
- ğŸš© Implementing a different approach than planned without discussing â€” flag to user
- ğŸš© Tests not implemented because "implementation works" â€” tests are in the checklist
- ğŸš© Declaring done without running self-verification â€” Phase 3 is mandatory
- ğŸš© Partial implementation claimed as complete â€” verify against the spec, not against "it compiles"

---

## When Complete

Report completion status with gate result.

**Next step:** `/x1-audit-plan` for independent verification (should find 0 gaps)
